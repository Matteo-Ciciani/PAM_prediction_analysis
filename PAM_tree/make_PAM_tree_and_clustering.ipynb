{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make PAM tree and clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import shutil\n",
    "import numpy as np\n",
    "import subprocess as sbp\n",
    "import pandas as pd\n",
    "from Bio import Seq, SeqIO\n",
    "import logomaker\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from Bio import SearchIO\n",
    "import math\n",
    "from scipy import stats\n",
    "from matplotlib import cm\n",
    "from matplotlib import colors\n",
    "import matplotlib.patches as mpatches\n",
    "import multiprocessing as mp\n",
    "from Bio import Phylo\n",
    "import matplotlib.image as image\n",
    "from Bio import Phylo\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read PAM distance matrix and normalize it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "workdir = 'PAM_clustering'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "ID = 98\n",
    "JS_matrix = pd.read_csv(os.path.join(workdir, 'all_PAMs_JS_div_{}_final.tsv'.format(ID)),\n",
    "                        sep='\\t', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "JS_matrix_norm = JS_matrix.divide(10) # 10 is the upper bound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_to_write = ['{}\\t{}\\t{}\\n'.format(eval(i)[0],\n",
    "    eval(i)[1], float(JS_matrix_norm.loc[i])) for i in JS_matrix_norm.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairwise_distance_matrix = os.path.join(workdir, 'pairwise_PAM_JS_matrix_final.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(pairwise_distance_matrix, 'w') as fh:\n",
    "    fh.write(''.join(matrix_to_write))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_file = os.path.join(workdir, 'JS_PAM_clusters_avg.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'usearch -cluster_aggd PAM_clustering/pairwise_PAM_JS_matrix_final.tsv -treeout PAM_clustering/PAM_tree_JS_avg_final.nwk -clusterout PAM_clustering/JS_PAM_clusters_avg.txt -id 0.62 -linkage avg'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "command = 'usearch -cluster_aggd {} -treeout {} -clusterout {} -id 0.62 -linkage avg'.format(\n",
    "    pairwise_distance_matrix, os.path.join(workdir, 'PAM_tree_JS_avg_final.nwk'), cluster_file)\n",
    "command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.system(command)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make PAM predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "IDs = [98]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "blastdir = 'Spacers_alignment'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_oriented_flanks = {ID:pd.read_csv(os.path.join(blastdir,\n",
    "    'realigned_oriented_flanking_regions_{}_clustering_new.tsv'.format(ID)),\n",
    "    sep='\\t', index_col=1) for ID in IDs}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ID in IDs:\n",
    "    all_oriented_flanks[ID] = all_oriented_flanks[ID].drop('Unnamed: 0', axis=1)\n",
    "    all_oriented_flanks[ID].columns = ['Upstream', 'Downstream']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "datadir = 'Cas9_tables'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "cas9_datasets = {ID:pd.read_csv(os.path.join(datadir,\n",
    "    'all_new_working_Cas9_clust_{}_oriented.tsv'.format(ID)),\n",
    "    sep='\\t', index_col=0) for ID in IDs}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pam_len = 30\n",
    "relevant_range = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "pam_predictions = {ID:{'Upstream':{}, 'Downstream':{}} for ID in IDs}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_info_df(seqs, pam_len, pos):\n",
    "    counters = [Counter() for i in range(pam_len)]\n",
    "    if pos == 'Downstream':\n",
    "        for i in range(len(counters)):\n",
    "            for seq in seqs:\n",
    "                if i < len(seq):\n",
    "                    if seq[i]!='N':\n",
    "                        counters[i][seq[i]] +=1\n",
    "    else:\n",
    "        for i in range(1,len(counters)+1):\n",
    "            for seq in seqs:\n",
    "                if i <= len(seq):\n",
    "                    if seq[-i]!='N':\n",
    "                        counters[-i][seq[-i]] +=1\n",
    "    freqs = pd.DataFrame(counters).fillna(0)\n",
    "    freqs = freqs.divide(freqs.sum(axis=1), axis=0).fillna(1.0/freqs.shape[1])\n",
    "    if not all(freqs.sum(axis=1).apply(lambda x: math.isclose(x, 1.0))):\n",
    "        raise ValueError\n",
    "    info = logomaker.transform_matrix(freqs, from_type='probability',\n",
    "        to_type='information')\n",
    "    return info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ID in IDs:\n",
    "    for i in all_oriented_flanks[ID].index:\n",
    "        up = all_oriented_flanks[ID].loc[i, 'Upstream']\n",
    "        down = all_oriented_flanks[ID].loc[i, 'Downstream']\n",
    "        if pd.notna(up):\n",
    "            pam_predictions[ID]['Upstream'][i] = make_info_df(up.split(','),\n",
    "                                                              pam_len, 'Upstream')\n",
    "        if pd.notna(down):\n",
    "            pam_predictions[ID]['Downstream'][i] = make_info_df(down.split(','),\n",
    "                                                                pam_len, 'Downstream')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_clusters = {ID:len(cas9_datasets[ID]['Cluster ID'].unique()) for ID in IDs}\n",
    "n_clusters_w_matches = {ID:all_oriented_flanks[ID].shape[0] for ID in IDs}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_n_flanking_seqs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_upstream_flanks = {ID:all_oriented_flanks[ID]['Upstream'].apply(\n",
    "    lambda x: len(x.split(',')) if pd.notna(x) else 0) for ID in IDs}\n",
    "n_downstream_flanks = {ID:all_oriented_flanks[ID]['Downstream'].apply(\n",
    "    lambda x: len(x.split(',')) if pd.notna(x) else 0) for ID in IDs}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "more_flanks_than_min = {ID:{'Upstream':n_upstream_flanks[ID]>=min_n_flanking_seqs,\n",
    "    'Downstream':n_downstream_flanks[ID]>=min_n_flanking_seqs} for ID in IDs}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_info_all_PAMs = {ID:{pos:{i:pam_predictions[ID][pos][i].sum(axis=1).values\n",
    "    for i in pam_predictions[ID][pos]} for pos in pam_predictions[ID]\n",
    "    } for ID in IDs}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_info_all_PAMs = {ID:{pos:{i:all_info_all_PAMs[ID][pos][i] for i in\n",
    "    all_info_all_PAMs[ID][pos] if more_flanks_than_min[ID][pos].loc[i]} for pos in\n",
    "    all_info_all_PAMs[ID]} for ID in IDs}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ids = {ID:np.unique([k for pos in all_info_all_PAMs[ID] for k in\n",
    "                     all_info_all_PAMs[ID][pos]]) for ID in IDs}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds = {ID:{k:min(2, max(1, pd.Series(np.concatenate(\n",
    "    [all_info_all_PAMs[ID][pos][k] for pos in all_info_all_PAMs[ID] if k\n",
    "     in all_info_all_PAMs[ID][pos]])).quantile(0.75)+1.5*stats.iqr(np.concatenate(\n",
    "    [all_info_all_PAMs[ID][pos][k] for pos in all_info_all_PAMs[ID] if k\n",
    "     in all_info_all_PAMs[ID][pos]])))) for k in all_ids[ID]} for ID in IDs}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "pam_predicted_positions = {ID:pd.DataFrame({k:{pos:any((all_info_all_PAMs[ID][\n",
    "    pos][k][0:relevant_range] if pos=='Downstream' else all_info_all_PAMs[ID][\n",
    "    pos][k][-relevant_range:])>=thresholds[ID][k]) if k in\n",
    "    all_info_all_PAMs[ID][pos] else False for pos in all_info_all_PAMs[ID]}\n",
    "    for k in all_ids[ID]}).transpose() for ID in IDs}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "has_prediction = {ID:pam_predicted_positions[ID][\n",
    "    pam_predicted_positions[ID].apply(lambda x: x[0]^x[1], axis=1)] for ID in IDs}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "high_confidence_PAM_positions = {ID:has_prediction[ID].apply(\n",
    "    lambda x: 'Upstream' if x[0] else 'Downstream', axis=1) for ID in IDs}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2546"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(high_confidence_PAM_positions[98])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering = pd.read_csv(cluster_file, sep='\\t', header=None,\n",
    "                         names=['Cluster', 'ID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering = clustering.groupby('Cluster')['ID'].apply(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "large_clusters = clustering[clustering.apply(len)>=20]\n",
    "len(large_clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2014"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "large_clusters.apply(len).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7910447761194029"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "large_clusters.apply(len).sum()/len(high_confidence_PAM_positions[98])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make consensus PAMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_seqs_reoriented = {}\n",
    "for i in large_clusters.index:\n",
    "    all_seqs_reoriented[i] = []\n",
    "    for Cas9 in large_clusters.loc[i]:\n",
    "        pos = high_confidence_PAM_positions[ID].loc[Cas9]\n",
    "        seqs = all_oriented_flanks[ID].loc[Cas9, pos].split(',')\n",
    "        if pos == 'Upstream':\n",
    "            seqs = [str(Seq.Seq(s).reverse_complement()) for s in seqs]\n",
    "        all_seqs_reoriented[i] += seqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_info_reoriented = {i:make_info_df(all_seqs_reoriented[i], pam_len, 'Downstream')\n",
    "                       for i in all_seqs_reoriented}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "within_cluster_avg_dist = {}\n",
    "for i in large_clusters.index:\n",
    "    pairs = [\"('{}', '{}')\".format(x,y) for x,y in itertools.combinations(\n",
    "        np.sort(large_clusters.loc[i]), 2)]\n",
    "    within_cluster_avg_dist[i] = JS_matrix.loc[pairs].median().iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sort clusters by size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "large_clusters = large_clusters[large_clusters.apply(len).sort_values(\n",
    "    ascending=False).index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
