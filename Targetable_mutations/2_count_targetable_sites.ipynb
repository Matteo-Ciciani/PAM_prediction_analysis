{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Count targetable sites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import shutil\n",
    "import numpy as np\n",
    "import subprocess as sbp\n",
    "import pandas as pd\n",
    "from Bio import Seq, SeqIO\n",
    "import logomaker\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from Bio import SearchIO\n",
    "import math\n",
    "from scipy import stats\n",
    "from matplotlib import cm\n",
    "from matplotlib import colors\n",
    "import matplotlib.patches as mpatches\n",
    "import multiprocessing as mp\n",
    "from Bio import Phylo\n",
    "import matplotlib.image as image\n",
    "from Bio import Align\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make PAM predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "IDs = [98]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "blastdir = 'Spacers_alignment'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_oriented_flanks = {ID:pd.read_csv(os.path.join(blastdir,\n",
    "    'realigned_oriented_flanking_regions_{}_clustering_new.tsv'.format(ID)),\n",
    "    sep='\\t', index_col=1) for ID in IDs}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ID in IDs:\n",
    "    all_oriented_flanks[ID] = all_oriented_flanks[ID].drop('Unnamed: 0', axis=1)\n",
    "    all_oriented_flanks[ID].columns = ['Upstream', 'Downstream']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "datadir = 'Cas9_tables'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "cas9_datasets = {ID:pd.read_csv(os.path.join(datadir,\n",
    "    'all_new_working_Cas9_clust_{}_oriented.tsv'.format(ID)),\n",
    "    sep='\\t', index_col=0) for ID in IDs}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pam_len = 30\n",
    "relevant_range = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pam_predictions = {ID:{'Upstream':{}, 'Downstream':{}} for ID in IDs}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_info_df(seqs, pam_len, pos):\n",
    "    counters = [Counter() for i in range(pam_len)]\n",
    "    if pos == 'Downstream':\n",
    "        for i in range(len(counters)):\n",
    "            for seq in seqs:\n",
    "                if i < len(seq):\n",
    "                    if seq[i]!='N':\n",
    "                        counters[i][seq[i]] +=1\n",
    "    else:\n",
    "        for i in range(1,len(counters)+1):\n",
    "            for seq in seqs:\n",
    "                if i <= len(seq):\n",
    "                    if seq[-i]!='N':\n",
    "                        counters[-i][seq[-i]] +=1\n",
    "    freqs = pd.DataFrame(counters).fillna(0)\n",
    "    freqs = freqs.divide(freqs.sum(axis=1), axis=0).fillna(1.0/freqs.shape[1])\n",
    "    if not all(freqs.sum(axis=1).apply(lambda x: math.isclose(x, 1.0))):\n",
    "        raise ValueError\n",
    "    info = logomaker.transform_matrix(freqs, from_type='probability',\n",
    "        to_type='information')\n",
    "    return info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ID in IDs:\n",
    "    for i in all_oriented_flanks[ID].index:\n",
    "        up = all_oriented_flanks[ID].loc[i, 'Upstream']\n",
    "        down = all_oriented_flanks[ID].loc[i, 'Downstream']\n",
    "        if pd.notna(up):\n",
    "            pam_predictions[ID]['Upstream'][i] = make_info_df(up.split(','),\n",
    "                                                              pam_len, 'Upstream')\n",
    "        if pd.notna(down):\n",
    "            pam_predictions[ID]['Downstream'][i] = make_info_df(down.split(','),\n",
    "                                                                pam_len, 'Downstream')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_clusters = {ID:len(cas9_datasets[ID]['Cluster ID'].unique()) for ID in IDs}\n",
    "n_clusters_w_matches = {ID:all_oriented_flanks[ID].shape[0] for ID in IDs}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_n_flanking_seqs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_upstream_flanks = {ID:all_oriented_flanks[ID]['Upstream'].apply(\n",
    "    lambda x: len(x.split(',')) if pd.notna(x) else 0) for ID in IDs}\n",
    "n_downstream_flanks = {ID:all_oriented_flanks[ID]['Downstream'].apply(\n",
    "    lambda x: len(x.split(',')) if pd.notna(x) else 0) for ID in IDs}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "more_flanks_than_min = {ID:{'Upstream':n_upstream_flanks[ID]>=min_n_flanking_seqs,\n",
    "    'Downstream':n_downstream_flanks[ID]>=min_n_flanking_seqs} for ID in IDs}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_info_all_PAMs = {ID:{pos:{i:pam_predictions[ID][pos][i].sum(axis=1).values\n",
    "    for i in pam_predictions[ID][pos]} for pos in pam_predictions[ID]\n",
    "    } for ID in IDs}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_info_all_PAMs = {ID:{pos:{i:all_info_all_PAMs[ID][pos][i] for i in\n",
    "    all_info_all_PAMs[ID][pos] if more_flanks_than_min[ID][pos].loc[i]} for pos in\n",
    "    all_info_all_PAMs[ID]} for ID in IDs}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ids = {ID:np.unique([k for pos in all_info_all_PAMs[ID] for k in\n",
    "                     all_info_all_PAMs[ID][pos]]) for ID in IDs}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds = {ID:{k:min(2, max(1, pd.Series(np.concatenate(\n",
    "    [all_info_all_PAMs[ID][pos][k] for pos in all_info_all_PAMs[ID] if k\n",
    "     in all_info_all_PAMs[ID][pos]])).quantile(0.75)+1.5*stats.iqr(np.concatenate(\n",
    "    [all_info_all_PAMs[ID][pos][k] for pos in all_info_all_PAMs[ID] if k\n",
    "     in all_info_all_PAMs[ID][pos]])))) for k in all_ids[ID]} for ID in IDs}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "pam_predicted_positions = {ID:pd.DataFrame({k:{pos:any((all_info_all_PAMs[ID][\n",
    "    pos][k][0:relevant_range] if pos=='Downstream' else all_info_all_PAMs[ID][\n",
    "    pos][k][-relevant_range:])>=thresholds[ID][k]) if k in\n",
    "    all_info_all_PAMs[ID][pos] else False for pos in all_info_all_PAMs[ID]}\n",
    "    for k in all_ids[ID]}).transpose() for ID in IDs}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "has_prediction = {ID:pam_predicted_positions[ID][\n",
    "    pam_predicted_positions[ID].apply(lambda x: x[0]^x[1], axis=1)] for ID in IDs}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "high_confidence_PAM_positions = {ID:has_prediction[ID].apply(\n",
    "    lambda x: 'Upstream' if x[0] else 'Downstream', axis=1) for ID in IDs}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2546"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(high_confidence_PAM_positions[98])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Turn PAMs into sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reverse_pam(pam):\n",
    "    newpam = pam[['A', 'T', 'C', 'G']][::-1]\n",
    "    newpam.columns = ['T', 'A', 'G', 'C']\n",
    "    newpam.index = newpam.index[::-1]\n",
    "    return newpam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logo2seq(ID, centroid):\n",
    "    pos = high_confidence_PAM_positions[ID].loc[centroid]\n",
    "    info = pam_predictions[ID][pos][centroid][0:10] if pos=='Downstream' else reverse_pam(\n",
    "        pam_predictions[ID][pos][centroid])[0:10]\n",
    "    threshold = thresholds[ID][centroid]\n",
    "    res = []\n",
    "    for i in info.index:\n",
    "        res.append([])\n",
    "        for base in info.columns:\n",
    "            if info.loc[i, base] >= threshold:\n",
    "                res[i].append(base)\n",
    "        if len(res[i])==0:\n",
    "            res[i].append('N')\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAM_seqs = {centroid:logo2seq(98, centroid) for centroid in high_confidence_PAM_positions[98].index}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do it recursively\n",
    "def make_PAM_seq(PAM):\n",
    "    if len(PAM) > 1:\n",
    "        return [b+x for b in PAM[0] for x in make_PAM_seq(PAM[1:])]\n",
    "    else:\n",
    "        return PAM[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAM_seqs_full = {centroid:make_PAM_seq(PAM_seqs[centroid]) for centroid in PAM_seqs}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAM_seqs_all_fw = [s for centroid in PAM_seqs_full for s in PAM_seqs_full[centroid]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAM_seqs_all_rev = [str(Seq.Seq(s).reverse_complement()) for s in PAM_seqs_all_fw]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAM_seqs_all = np.unique(PAM_seqs_all_fw + PAM_seqs_all_rev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAM_non_N_count = [10-s.count('N') for s in PAM_seqs_all]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read ClinVar dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "clinvar_data = pd.read_csv('filtered_variant_summary.tsv', sep='\\t', index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find PAMs in mutated alleles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "clinvar_data_short = clinvar_data[clinvar_data['Length']<10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate sequence containing the wt and mutated allele\n",
    "# take 6 up + allele + 6 down\n",
    "# check if pam on mut and not on wt\n",
    "ref_target = []\n",
    "alt_target = []\n",
    "for i in clinvar_data_short.index:\n",
    "    up = clinvar_data_short.loc[i, 'Upstream'][-6:]\n",
    "    down = clinvar_data_short.loc[i, 'Downstream'][0:6]\n",
    "    ref = clinvar_data_short.loc[i, 'ReferenceAlleleVCF']\n",
    "    alt = clinvar_data_short.loc[i, 'AlternateAlleleVCF']\n",
    "    ref_target.append(up + ref + down)\n",
    "    alt_target.append(up + alt + down)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "clinvar_data_flank = clinvar_data_short.assign(**{'RefAlleleFlank':ref_target,\n",
    "                                                  'AltAlleleFlank':alt_target})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "aligner = Align.PairwiseAligner()\n",
    "aligner.mode = 'local'\n",
    "aligner.open_gap_score = -10\n",
    "aligner.extend_gap_score = -10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000 loci processed...\n",
      "20000 loci processed...\n",
      "30000 loci processed...\n",
      "40000 loci processed...\n",
      "50000 loci processed...\n",
      "60000 loci processed...\n",
      "70000 loci processed...\n",
      "80000 loci processed...\n",
      "90000 loci processed...\n",
      "100000 loci processed...\n",
      "110000 loci processed...\n",
      "120000 loci processed...\n"
     ]
    }
   ],
   "source": [
    "has_match_only_alt = []\n",
    "for m,i in enumerate(clinvar_data_flank.index):\n",
    "    has_match = False\n",
    "    if m%10000==0 and m!=0: print('{} loci processed...'.format(m))\n",
    "    ref = clinvar_data_flank.loc[i, 'RefAlleleFlank']\n",
    "    alt = clinvar_data_flank.loc[i, 'AltAlleleFlank']\n",
    "    if type(ref)==str and type(alt)==str: # there's a couple of NaNs, ignore them\n",
    "        for n,PAM in enumerate(PAM_seqs_all):\n",
    "            alt_aln_score = aligner.align(alt, PAM).score\n",
    "            # if match on alt, check if no match on ref\n",
    "            if alt_aln_score == PAM_non_N_count[n]:\n",
    "                ref_aln_score = aligner.align(ref, PAM).score\n",
    "                if ref_aln_score < PAM_non_N_count[n]:\n",
    "                    has_match = True\n",
    "                    break\n",
    "    has_match_only_alt.append(has_match)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "clinvar_data_flank = clinvar_data_flank.assign(**{'Target on ALT allele':has_match_only_alt})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9855673126644902"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fraction on sites with alt allele targetable\n",
    "clinvar_data_flank['Target on ALT allele'].sum()/clinvar_data_flank.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
